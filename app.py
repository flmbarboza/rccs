import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime
import plotly.express as px
import plotly.graph_objects as go
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

class CreditScoringAI:
    def __init__(self):
        self.model_suggestions = {
            "logistic_regression": "Regress√£o Log√≠stica - Modelo interpret√°vel, ideal para explicar decis√µes de cr√©dito",
            "random_forest": "Random Forest - Modelo robusto que captura intera√ß√µes complexas entre vari√°veis",
            "gradient_boosting": "Gradient Boosting - Excelente performance, mas requer mais cuidado com overfitting"
        }
    
    def analyze_data_quality(self, df):
        """An√°lise completa da qualidade dos dados com sugest√µes da IA"""
        quality_report = {
            'problemas_detectados': [],
            'sugestoes_nomes': {},
            'sugestoes_valores': {},
            'sugestoes_fusao': [],
            'sugestoes_preprocessamento': []
        }
        
        for col in df.columns:
            # An√°lise de nomes das colunas
            if ' ' in col or col.isupper() or any(char in col for char in ['/', '(', ')', '-']):
                new_name = col.lower().replace(' ', '_').replace('/', '_').replace('(', '').replace(')', '').replace('-', '_')
                quality_report['sugestoes_nomes'][col] = {
                    'novo_nome': new_name,
                    'motivo': 'Padroniza√ß√£o: min√∫sculas, sem espa√ßos ou caracteres especiais'
                }
            
            # An√°lise de valores missing
            missing_pct = df[col].isnull().sum() / len(df) * 100
            if missing_pct > 5:
                quality_report['problemas_detectados'].append({
                    'coluna': col,
                    'problema': f'{missing_pct:.1f}% de dados faltantes',
                    'severidade': 'Alta' if missing_pct > 20 else 'M√©dia',
                    'sugestao': self._suggest_missing_treatment(df, col, missing_pct)
                })
            
            # An√°lise de inconsist√™ncias
            if df[col].dtype == 'object':
                # Detectar poss√≠veis inconsist√™ncias em categ√≥ricas
                value_counts = df[col].value_counts()
                similar_values = self._detect_similar_values(value_counts.index.tolist())
                if similar_values:
                    quality_report['sugestoes_valores'][col] = similar_values
            
            elif df[col].dtype in ['int64', 'float64']:
                # Detectar outliers
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                outliers = df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)][col]
                
                if len(outliers) > len(df) * 0.05:  # Mais de 5% outliers
                    quality_report['problemas_detectados'].append({
                        'coluna': col,
                        'problema': f'{len(outliers)} outliers detectados ({len(outliers)/len(df)*100:.1f}%)',
                        'severidade': 'M√©dia',
                        'sugestao': f'Considere winsoriza√ß√£o ou transforma√ß√£o log para {col}'
                    })
        
        # Sugest√µes de fus√£o de vari√°veis
        quality_report['sugestoes_fusao'] = self._suggest_variable_fusion(df)
        
        # Sugest√µes de pr√©-processamento
        quality_report['sugestoes_preprocessamento'] = self._suggest_preprocessing(df)
        
        return quality_report
    
    def _suggest_missing_treatment(self, df, col, missing_pct):
        """Sugere tratamento para dados faltantes baseado no tipo e padr√£o"""
        if df[col].dtype in ['int64', 'float64']:
            if missing_pct > 30:
                return f"Alto percentual missing. Considere: criar flag 'is_{col}_missing' e imputar com mediana"
            else:
                return f"Imputar com mediana ou usar algoritmos que lidam com missing (Random Forest)"
        else:
            if missing_pct > 30:
                return f"Criar categoria 'N√£o_Informado' ou remover vari√°vel se n√£o for cr√≠tica"
            else:
                return f"Imputar com moda ou criar categoria 'Outros'"
    
    def _detect_similar_values(self, values):
        """Detecta valores similares que podem ser padronizados"""
        suggestions = {}
        
        # Detectar varia√ß√µes de caso
        values_lower = [str(v).lower() for v in values if pd.notna(v)]
        value_groups = {}
        
        for val in values:
            if pd.isna(val):
                continue
            val_lower = str(val).lower()
            if val_lower not in value_groups:
                value_groups[val_lower] = []
            value_groups[val_lower].append(val)
        
        # Encontrar grupos com m√∫ltiplas varia√ß√µes
        for key, group in value_groups.items():
            if len(group) > 1:
                suggestions[f"Grupo '{key}'"] = {
                    'valores_originais': group,
                    'valor_sugerido': max(group, key=len),  # Valor mais longo como padr√£o
                    'motivo': 'Varia√ß√µes de capitaliza√ß√£o detectadas'
                }
        
        return suggestions
    
    def _suggest_variable_fusion(self, df):
        """Sugere poss√≠veis fus√µes de vari√°veis"""
        fusion_suggestions = []
        
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        # Sugerir fus√µes baseadas em correla√ß√£o alta
        if len(numeric_cols) > 1:
            corr_matrix = df[numeric_cols].corr()
            high_corr_pairs = []
            
            for i in range(len(corr_matrix.columns)):
                for j in range(i+1, len(corr_matrix.columns)):
                    if abs(corr_matrix.iloc[i, j]) > 0.8:
                        col1, col2 = corr_matrix.columns[i], corr_matrix.columns[j]
                        high_corr_pairs.append((col1, col2, corr_matrix.iloc[i, j]))
            
            for col1, col2, corr in high_corr_pairs:
                fusion_suggestions.append({
                    'tipo': 'fusao_alta_correlacao',
                    'variaveis': [col1, col2],
                    'correlacao': corr,
                    'sugestao': f'Criar √≠ndice combinado: ({col1} + {col2})/2 ou usar PCA',
                    'motivo': f'Correla√ß√£o muito alta ({corr:.3f}) pode causar multicolinearidade'
                })
        
        # Sugerir cria√ß√£o de vari√°veis derivadas
        if 'idade' in df.columns:
            fusion_suggestions.append({
                'tipo': 'variavel_derivada',
                'variaveis': ['idade'],
                'sugestao': 'Criar faixas et√°rias: Jovem(18-30), Adulto(31-50), S√™nior(51+)',
                'motivo': 'Rela√ß√£o n√£o-linear com risco de cr√©dito'
            })
        
        if 'renda' in df.columns:
            fusion_suggestions.append({
                'tipo': 'variavel_derivada',
                'variaveis': ['renda'],
                'sugestao': 'Criar classes de renda ou aplicar transforma√ß√£o log',
                'motivo': 'Distribui√ß√£o geralmente assim√©trica'
            })
        
        return fusion_suggestions
    
    def _suggest_preprocessing(self, df):
        """Sugest√µes gerais de pr√©-processamento"""
        suggestions = []
        
        # Encoding de categ√≥ricas
        categorical_cols = df.select_dtypes(include=['object']).columns
        if len(categorical_cols) > 0:
            suggestions.append({
                'etapa': 'encoding_categoricas',
                'colunas': list(categorical_cols),
                'sugestao': 'Aplicar One-Hot Encoding ou Target Encoding',
                'detalhes': 'One-Hot para poucas categorias, Target Encoding para muitas'
            })
        
        # Normaliza√ß√£o
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            suggestions.append({
                'etapa': 'normalizacao',
                'colunas': list(numeric_cols),
                'sugestao': 'Aplicar StandardScaler ou MinMaxScaler',
                'detalhes': 'Especialmente importante para Regress√£o Log√≠stica'
            })
        
        # Feature Selection
        if len(df.columns) > 20:
            suggestions.append({
                'etapa': 'selecao_features',
                'colunas': list(df.columns),
                'sugestao': 'Aplicar sele√ß√£o de features',
                'detalhes': 'Use RFE, SelectKBest ou import√¢ncia por √°rvores'
            })
        
        return suggestions
    
    def apply_data_corrections(self, df, corrections_config):
        """Aplica corre√ß√µes baseadas na configura√ß√£o do usu√°rio"""
        df_corrected = df.copy()
        
        # Renomear colunas
        if 'rename_columns' in corrections_config:
            df_corrected = df_corrected.rename(columns=corrections_config['rename_columns'])
        
        # Tratar valores faltantes
        if 'missing_treatment' in corrections_config:
            for col, method in corrections_config['missing_treatment'].items():
                if method == 'median':
                    df_corrected[col].fillna(df_corrected[col].median(), inplace=True)
                elif method == 'mode':
                    df_corrected[col].fillna(df_corrected[col].mode()[0], inplace=True)
                elif method == 'flag_and_median':
                    df_corrected[f'{col}_missing'] = df_corrected[col].isnull().astype(int)
                    df_corrected[col].fillna(df_corrected[col].median(), inplace=True)
                elif method == 'category_missing':
                    df_corrected[col].fillna('N√£o_Informado', inplace=True)
        
        # Padronizar valores categ√≥ricos
        if 'standardize_values' in corrections_config:
            for col, mappings in corrections_config['standardize_values'].items():
                df_corrected[col] = df_corrected[col].replace(mappings)
        
        # Tratar outliers
        if 'outlier_treatment' in corrections_config:
            for col, method in corrections_config['outlier_treatment'].items():
                if method == 'winsorize':
                    Q1 = df_corrected[col].quantile(0.05)
                    Q3 = df_corrected[col].quantile(0.95)
                    df_corrected[col] = df_corrected[col].clip(lower=Q1, upper=Q3)
                elif method == 'log_transform':
                    df_corrected[col] = np.log1p(df_corrected[col])
        
        # Criar vari√°veis derivadas
        if 'create_derived' in corrections_config:
            for config in corrections_config['create_derived']:
                if config['type'] == 'age_groups':
                    col = config['column']
                    df_corrected[f'{col}_grupo'] = pd.cut(
                        df_corrected[col], 
                        bins=[0, 30, 50, 100], 
                        labels=['Jovem', 'Adulto', 'Senior']
                    )
                elif config['type'] == 'income_log':
                    col = config['column']
                    df_corrected[f'{col}_log'] = np.log1p(df_corrected[col])
        
        return df_corrected
        
    def analyze_univariate(self, df, column):
        """An√°lise univariada automatizada com sugest√µes da IA"""
        analysis = {}
        
        if df[column].dtype in ['int64', 'float64']:
            # Vari√°vel num√©rica
            analysis['tipo'] = 'num√©rica'
            analysis['estatisticas'] = {
                'm√©dia': df[column].mean(),
                'mediana': df[column].median(),
                'desvio_padr√£o': df[column].std(),
                'm√≠n': df[column].min(),
                'm√°x': df[column].max(),
                'missing': df[column].isnull().sum()
            }
            
            # Sugest√µes da IA
            cv = analysis['estatisticas']['desvio_padr√£o'] / analysis['estatisticas']['m√©dia']
            if cv > 1:
                analysis['sugestao_ia'] = f"‚ö†Ô∏è A vari√°vel '{column}' apresenta alta variabilidade (CV={cv:.2f}). Considere transforma√ß√µes como log ou binning para melhorar a modelagem."
            elif analysis['estatisticas']['missing'] > len(df) * 0.1:
                analysis['sugestao_ia'] = f"‚ö†Ô∏è A vari√°vel '{column}' possui {analysis['estatisticas']['missing']} valores missing ({analysis['estatisticas']['missing']/len(df)*100:.1f}%). Considere estrat√©gias de imputa√ß√£o ou remo√ß√£o."
            else:
                analysis['sugestao_ia'] = f"‚úÖ A vari√°vel '{column}' apresenta distribui√ß√£o adequada para modelagem."
                
        else:
            # Vari√°vel categ√≥rica
            analysis['tipo'] = 'categ√≥rica'
            analysis['estatisticas'] = {
                'categorias': df[column].nunique(),
                'moda': df[column].mode().iloc[0] if not df[column].mode().empty else 'N/A',
                'missing': df[column].isnull().sum(),
                'distribuicao': df[column].value_counts().to_dict()
            }
            
            # Sugest√µes da IA
            if analysis['estatisticas']['categorias'] > 10:
                analysis['sugestao_ia'] = f"‚ö†Ô∏è A vari√°vel '{column}' possui muitas categorias ({analysis['estatisticas']['categorias']}). Considere agrupamento das categorias menos frequentes."
            else:
                analysis['sugestao_ia'] = f"‚úÖ A vari√°vel '{column}' possui n√∫mero adequado de categorias para modelagem."
                
        return analysis
    
    def analyze_bivariate(self, df, var1, var2, target=None):
        """An√°lise bivariada com insights da IA"""
        analysis = {}
        
        # Correla√ß√£o se ambas num√©ricas
        if df[var1].dtype in ['int64', 'float64'] and df[var2].dtype in ['int64', 'float64']:
            correlation = df[var1].corr(df[var2])
            analysis['correlacao'] = correlation
            
            if abs(correlation) > 0.7:
                analysis['sugestao_ia'] = f"‚ö†Ô∏è Alta correla√ß√£o ({correlation:.3f}) entre {var1} e {var2}. Considere remover uma das vari√°veis para evitar multicolinearidade."
            elif abs(correlation) > 0.3:
                analysis['sugestao_ia'] = f"üìä Correla√ß√£o moderada ({correlation:.3f}) entre {var1} e {var2}. Ambas podem ser √∫teis no modelo."
            else:
                analysis['sugestao_ia'] = f"‚úÖ Baixa correla√ß√£o ({correlation:.3f}) entre {var1} e {var2}. Vari√°veis independentes."
        
        # An√°lise com target se dispon√≠vel
        if target and target in df.columns:
            if df[var1].dtype in ['int64', 'float64']:
                target_corr = df[var1].corr(df[target])
                analysis['correlacao_target'] = target_corr
                
                if abs(target_corr) > 0.3:
                    analysis['sugestao_ia'] += f"\nüéØ Excelente: {var1} tem boa correla√ß√£o com o target ({target_corr:.3f}). Vari√°vel preditiva importante!"
                else:
                    analysis['sugestao_ia'] += f"\nüìà {var1} tem correla√ß√£o baixa com o target ({target_corr:.3f}). Pode n√£o ser muito preditiva."
        
        return analysis
    
    def suggest_model_strategy(self, df, target_column):
        """Sugest√µes estrat√©gicas da IA para modelagem"""
        suggestions = []
        
        # An√°lise do dataset
        n_rows, n_cols = df.shape
        missing_pct = df.isnull().sum().sum() / (n_rows * n_cols) * 100
        
        # An√°lise do target
        if target_column in df.columns:
            target_balance = df[target_column].value_counts()
            minority_pct = target_balance.min() / target_balance.sum() * 100
            
            if minority_pct < 10:
                suggestions.append(f"‚ö†Ô∏è Dataset desbalanceado ({minority_pct:.1f}% classe minorit√°ria). Considere t√©cnicas de balanceamento como SMOTE ou ajuste de pesos.")
            
            if n_rows < 1000:
                suggestions.append("üìä Dataset pequeno. Regress√£o Log√≠stica pode ser mais est√°vel que modelos complexos.")
            elif n_rows > 50000:
                suggestions.append("üöÄ Dataset grande. Modelos como Random Forest ou Gradient Boosting podem capturar padr√µes complexos.")
        
        if missing_pct > 15:
            suggestions.append(f"üîß Alto percentual de dados faltantes ({missing_pct:.1f}%). Implemente estrat√©gia robusta de imputa√ß√£o.")
        
        if n_cols > 50:
            suggestions.append("üéØ Muitas vari√°veis. Considere sele√ß√£o de features com t√©cnicas como RFE ou import√¢ncia por √°rvores.")
        
        return suggestions
    
    def generate_performance_insights(self, y_true, y_pred, y_proba=None):
        """Insights de performance do modelo"""
        insights = []
        
        # AUC Score
        if y_proba is not None:
            auc = roc_auc_score(y_true, y_proba)
            if auc > 0.8:
                insights.append(f"üéâ Excelente performance! AUC = {auc:.3f}. O modelo tem alta capacidade discriminat√≥ria.")
            elif auc > 0.7:
                insights.append(f"üëç Boa performance. AUC = {auc:.3f}. O modelo √© adequado para uso em produ√ß√£o.")
            elif auc > 0.6:
                insights.append(f"‚ö†Ô∏è Performance moderada. AUC = {auc:.3f}. Considere engenharia de features ou outros algoritmos.")
            else:
                insights.append(f"‚ùå Performance baixa. AUC = {auc:.3f}. O modelo precisa ser reformulado.")
        
        # Matriz de confus√£o
        cm = confusion_matrix(y_true, y_pred)
        tn, fp, fn, tp = cm.ravel()
        
        # M√©tricas de neg√≥cio
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        
        insights.append(f"üìä Precis√£o: {precision:.3f} | Recall: {recall:.3f}")
        
        # An√°lise de neg√≥cio
        if fp > fn:
            insights.append("üí∞ Modelo conservador: mais falsos positivos (rejeita bons clientes). Considere ajustar threshold para reduzir perdas de neg√≥cio.")
        elif fn > fp:
            insights.append("‚ö†Ô∏è Modelo arriscado: mais falsos negativos (aprova maus clientes). Considere aumentar threshold para reduzir inadimpl√™ncia.")
        
        return insights
    
    def suggest_credit_policies(self, model_performance, business_context):
        """Sugest√µes de pol√≠ticas de cr√©dito baseadas na IA"""
        policies = []
        
        policies.append("üéØ **Pol√≠ticas de Aprova√ß√£o Recomendadas:**")
        
        if business_context.get('risk_appetite', 'medium') == 'low':
            policies.append("- Threshold conservador: Score > 0.7 para aprova√ß√£o autom√°tica")
            policies.append("- An√°lise manual para scores entre 0.5-0.7")
            policies.append("- Rejei√ß√£o autom√°tica para scores < 0.5")
        elif business_context.get('risk_appetite', 'medium') == 'high':
            policies.append("- Threshold agressivo: Score > 0.4 para aprova√ß√£o autom√°tica")
            policies.append("- An√°lise manual apenas para scores < 0.3")
        else:
            policies.append("- Threshold moderado: Score > 0.6 para aprova√ß√£o autom√°tica")
            policies.append("- An√°lise manual para scores entre 0.4-0.6")
            policies.append("- Rejei√ß√£o autom√°tica para scores < 0.4")
        
        policies.append("\nüìã **Recomenda√ß√µes de Monitoramento:**")
        policies.append("- Revisar modelo mensalmente")
        policies.append("- Monitorar PSI (Population Stability Index)")
        policies.append("- Acompanhar taxa de aprova√ß√£o e inadimpl√™ncia")
        
        return policies

def main():
    st.set_page_config(page_title="Assistente IA - Credit Scoring", layout="wide")
    
    st.title("ü§ñ Assistente IA para An√°lise de Credit Scoring")
    st.markdown("*Seu assistente inteligente para an√°lise de risco de cr√©dito*")
    
    # Inicializar IA
    if 'ai_assistant' not in st.session_state:
        st.session_state.ai_assistant = CreditScoringAI()
    
    ai = st.session_state.ai_assistant
    
    # Sidebar para upload e configura√ß√µes
    with st.sidebar:
        st.header("üìä Configura√ß√µes")
        
        uploaded_file = st.file_uploader("Upload do dataset", type=['csv'])
        
        if uploaded_file:
            df = pd.read_csv(uploaded_file)
            st.success(f"Dataset carregado: {df.shape[0]} linhas, {df.shape[1]} colunas")
            
            # Sele√ß√£o da vari√°vel target
            target_col = st.selectbox("Selecione a vari√°vel target (inadimpl√™ncia):", df.columns)
            
            # Contexto de neg√≥cio
            st.subheader("üè¶ Contexto de Neg√≥cio")
            risk_appetite = st.selectbox("Apetite ao risco:", ["low", "medium", "high"])
            business_context = {"risk_appetite": risk_appetite}
    
    # Tabs principais
    if 'df' in locals():
        tab0, tab1, tab2, tab3, tab4, tab5 = st.tabs(["üîß Qualidade dos Dados", "üìä An√°lise Univariada", "üîó An√°lise Bivariada", "ü§ñ Modelagem", "üìà Performance", "üìã Pol√≠ticas"])
        
        with tab0:
            st.header("üîß An√°lise e Corre√ß√£o da Qualidade dos Dados")
            
            # An√°lise autom√°tica da qualidade
            if st.button("üîç Analisar Qualidade dos Dados"):
                with st.spinner("Analisando qualidade dos dados..."):
                    quality_report = ai.analyze_data_quality(df)
                    st.session_state.quality_report = quality_report
                    st.success("‚úÖ An√°lise de qualidade conclu√≠da!")
            
            if 'quality_report' in st.session_state:
                report = st.session_state.quality_report
                
                # Problemas detectados
                if report['problemas_detectados']:
                    st.subheader("‚ö†Ô∏è Problemas Detectados")
                    for problema in report['problemas_detectados']:
                        with st.expander(f"{problema['coluna']}: {problema['problema']} - Severidade: {problema['severidade']}"):
                            st.write(f"**Sugest√£o da IA:** {problema['sugestao']}")
                
                # Painel interativo de corre√ß√µes
                st.subheader("üõ†Ô∏è Painel de Corre√ß√µes Interativo")
                
                corrections_config = {}
                
                # Renomear colunas
                if report['sugestoes_nomes']:
                    st.write("**üìù Renomear Colunas:**")
                    rename_dict = {}
                    for old_name, suggestion in report['sugestoes_nomes'].items():
                        col1, col2, col3 = st.columns([2, 2, 1])
                        col1.write(f"**{old_name}**")
                        new_name = col2.text_input(f"Novo nome", suggestion['novo_nome'], key=f"rename_{old_name}")
                        if col3.checkbox("Aplicar", key=f"apply_rename_{old_name}"):
                            rename_dict[old_name] = new_name
                    if rename_dict:
                        corrections_config['rename_columns'] = rename_dict
                
                # Tratar dados faltantes
                st.write("**üîç Tratamento de Dados Faltantes:**")
                missing_treatment = {}
                for col in df.columns:
                    missing_pct = df[col].isnull().sum() / len(df) * 100
                    if missing_pct > 0:
                        col1, col2, col3 = st.columns([2, 2, 1])
                        col1.write(f"**{col}** ({missing_pct:.1f}% missing)")
                        
                        if df[col].dtype in ['int64', 'float64']:
                            options = ['N√£o tratar', 'Mediana', 'Flag + Mediana']
                            method = col2.selectbox("M√©todo", options, key=f"missing_{col}")
                            if col3.checkbox("Aplicar", key=f"apply_missing_{col}") and method != 'N√£o tratar':
                                if method == 'Mediana':
                                    missing_treatment[col] = 'median'
                                elif method == 'Flag + Mediana':
                                    missing_treatment[col] = 'flag_and_median'
                        else:
                            options = ['N√£o tratar', 'Moda', 'Categoria "N√£o_Informado"']
                            method = col2.selectbox("M√©todo", options, key=f"missing_{col}")
                            if col3.checkbox("Aplicar", key=f"apply_missing_{col}") and method != 'N√£o tratar':
                                if method == 'Moda':
                                    missing_treatment[col] = 'mode'
                                elif method == 'Categoria "N√£o_Informado"':
                                    missing_treatment[col] = 'category_missing'
                
                if missing_treatment:
                    corrections_config['missing_treatment'] = missing_treatment
                
                # Padronizar valores categ√≥ricos
                if report['sugestoes_valores']:
                    st.write("**üî§ Padroniza√ß√£o de Valores Categ√≥ricos:**")
                    standardize_values = {}
                    for col, suggestions in report['sugestoes_valores'].items():
                        st.write(f"**Coluna: {col}**")
                        col_mappings = {}
                        for group_name, group_info in suggestions.items():
                            with st.expander(f"{group_name}"):
                                st.write(f"Valores encontrados: {group_info['valores_originais']}")
                                new_value = st.text_input(f"Valor padronizado", group_info['valor_sugerido'], key=f"std_{col}_{group_name}")
                                if st.checkbox(f"Aplicar padroniza√ß√£o", key=f"apply_std_{col}_{group_name}"):
                                    for orig_val in group_info['valores_originais']:
                                        col_mappings[orig_val] = new_value
                        if col_mappings:
                            standardize_values[col] = col_mappings
                    
                    if standardize_values:
                        corrections_config['standardize_values'] = standardize_values
                
                # Tratamento de outliers
                st.write("**üìä Tratamento de Outliers:**")
                outlier_treatment = {}
                numeric_cols = df.select_dtypes(include=[np.number]).columns
                for col in numeric_cols:
                    Q1 = df[col].quantile(0.25)
                    Q3 = df[col].quantile(0.75)
                    IQR = Q3 - Q1
                    outliers = df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)]
                    
                    if len(outliers) > 0:
                        col1, col2, col3 = st.columns([2, 2, 1])
                        col1.write(f"**{col}** ({len(outliers)} outliers)")
                        method = col2.selectbox("M√©todo", ['N√£o tratar', 'Winsoriza√ß√£o', 'Log Transform'], key=f"outlier_{col}")
                        if col3.checkbox("Aplicar", key=f"apply_outlier_{col}") and method != 'N√£o tratar':
                            if method == 'Winsoriza√ß√£o':
                                outlier_treatment[col] = 'winsorize'
                            elif method == 'Log Transform':
                                outlier_treatment[col] = 'log_transform'
                
                if outlier_treatment:
                    corrections_config['outlier_treatment'] = outlier_treatment
                
                # Cria√ß√£o de vari√°veis derivadas
                if report['sugestoes_fusao']:
                    st.write("**üîó Cria√ß√£o de Vari√°veis Derivadas:**")
                    create_derived = []
                    for suggestion in report['sugestoes_fusao']:
                        if suggestion['tipo'] == 'variavel_derivada':
                            with st.expander(f"Sugest√£o: {suggestion['sugestao']}"):
                                st.write(f"**Motivo:** {suggestion['motivo']}")
                                if st.checkbox(f"Criar vari√°vel derivada", key=f"derived_{suggestion['variaveis'][0]}"):
                                    if 'idade' in suggestion['variaveis']:
                                        create_derived.append({'type': 'age_groups', 'column': 'idade'})
                                    elif 'renda' in suggestion['variaveis']:
                                        create_derived.append({'type': 'income_log', 'column': 'renda'})
                    
                    if create_derived:
                        corrections_config['create_derived'] = create_derived
                
                # Bot√£o para aplicar todas as corre√ß√µes
                if st.button("üöÄ Aplicar Todas as Corre√ß√µes Selecionadas"):
                    if corrections_config:
                        with st.spinner("Aplicando corre√ß√µes..."):
                            df_corrected = ai.apply_data_corrections(df, corrections_config)
                            st.session_state.df_corrected = df_corrected
                            st.session_state.corrections_applied = corrections_config
                            st.success("‚úÖ Corre√ß√µes aplicadas com sucesso!")
                            
                            # Mostrar resumo das mudan√ßas
                            st.subheader("üìã Resumo das Corre√ß√µes Aplicadas:")
                            for key, value in corrections_config.items():
                                if key == 'rename_columns':
                                    st.write(f"‚Ä¢ **Colunas renomeadas:** {len(value)} colunas")
                                elif key == 'missing_treatment':
                                    st.write(f"‚Ä¢ **Dados faltantes tratados:** {len(value)} colunas")
                                elif key == 'standardize_values':
                                    st.write(f"‚Ä¢ **Valores padronizados:** {len(value)} colunas")
                                elif key == 'outlier_treatment':
                                    st.write(f"‚Ä¢ **Outliers tratados:** {len(value)} colunas")
                                elif key == 'create_derived':
                                    st.write(f"‚Ä¢ **Vari√°veis derivadas criadas:** {len(value)} vari√°veis")
                    else:
                        st.warning("Nenhuma corre√ß√£o foi selecionada!")
                
                # Mostrar compara√ß√£o antes/depois
                if 'df_corrected' in st.session_state:
                    st.subheader("üìä Compara√ß√£o: Antes vs. Depois")
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        st.write("**Dataset Original:**")
                        st.write(f"‚Ä¢ Linhas: {df.shape[0]}")
                        st.write(f"‚Ä¢ Colunas: {df.shape[1]}")
                        st.write(f"‚Ä¢ Missing total: {df.isnull().sum().sum()}")
                        st.write(f"‚Ä¢ Tipos de dados: {df.dtypes.value_counts().to_dict()}")
                    
                    with col2:
                        st.write("**Dataset Corrigido:**")
                        df_corr = st.session_state.df_corrected
                        st.write(f"‚Ä¢ Linhas: {df_corr.shape[0]}")
                        st.write(f"‚Ä¢ Colunas: {df_corr.shape[1]}")
                        st.write(f"‚Ä¢ Missing total: {df_corr.isnull().sum().sum()}")
                        st.write(f"‚Ä¢ Tipos de dados: {df_corr.dtypes.value_counts().to_dict()}")
                    
                    # Op√ß√£o de usar dataset corrigido nas pr√≥ximas an√°lises
                    if st.checkbox("üîÑ Usar dataset corrigido nas pr√≥ximas an√°lises"):
                        st.session_state.df_to_use = st.session_state.df_corrected
                        st.info("‚úÖ Dataset corrigido ser√° usado nas pr√≥ximas an√°lises!")
                    else:
                        st.session_state.df_to_use = df
            
            else:
                st.info("üëÜ Clique em 'Analisar Qualidade dos Dados' para come√ßar")
        
        # Usar dataset corrigido se dispon√≠vel
        df_analysis = st.session_state.get('df_to_use', df)
        
        with tab1:
            st.header("üìä An√°lise Univariada com IA")
            
            col1, col2 = st.columns([1, 2])
            
            with col1:
                selected_var = st.selectbox("Selecione uma vari√°vel:", df_analysis.columns)
                
                if st.button("üîç Analisar com IA"):
                    analysis = ai.analyze_univariate(df_analysis, selected_var)
                    
                    st.subheader(f"An√°lise: {selected_var}")
                    st.write(f"**Tipo:** {analysis['tipo']}")
                    
                    # Estat√≠sticas
                    stats_df = pd.DataFrame(analysis['estatisticas'], index=[0]).T
                    st.write("**Estat√≠sticas:**")
                    st.dataframe(stats_df)
                    
                    # Sugest√£o da IA
                    st.write("**üí° Sugest√£o da IA:**")
                    st.info(analysis['sugestao_ia'])
            
            with col2:
                if 'selected_var' in locals():
                    # Gr√°fico
                    if df_analysis[selected_var].dtype in ['int64', 'float64']:
                        fig = px.histogram(df_analysis, x=selected_var, title=f"Distribui√ß√£o de {selected_var}")
                    else:
                        fig = px.bar(df_analysis[selected_var].value_counts().reset_index(), 
                                   x='index', y=selected_var, title=f"Frequ√™ncia de {selected_var}")
                    st.plotly_chart(fig, use_container_width=True)
        
        with tab2:
            st.header("üîó An√°lise Bivariada com IA")
            
            col1, col2 = st.columns([1, 2])
            
            with col1:
                var1 = st.selectbox("Primeira vari√°vel:", df_analysis.columns)
                var2 = st.selectbox("Segunda vari√°vel:", df_analysis.columns)
                
                if st.button("üîç An√°lise Bivariada"):
                    analysis = ai.analyze_bivariate(df_analysis, var1, var2, target_col)
                    
                    st.subheader(f"An√°lise: {var1} vs {var2}")
                    
                    if 'correlacao' in analysis:
                        st.metric("Correla√ß√£o", f"{analysis['correlacao']:.3f}")
                    
                    if 'correlacao_target' in analysis:
                        st.metric("Correla√ß√£o com Target", f"{analysis['correlacao_target']:.3f}")
                    
                    st.write("**üí° Insights da IA:**")
                    st.info(analysis['sugestao_ia'])
            
            with col2:
                if 'var1' in locals() and 'var2' in locals():
                    if df_analysis[var1].dtype in ['int64', 'float64'] and df_analysis[var2].dtype in ['int64', 'float64']:
                        fig = px.scatter(df_analysis, x=var1, y=var2, color=target_col if target_col else None)
                    else:
                        # Crosstab para categ√≥ricas
                        crosstab = pd.crosstab(df_analysis[var1], df_analysis[var2])
                        fig = px.imshow(crosstab, text_auto=True)
                    st.plotly_chart(fig, use_container_width=True)= st.selectbox("Primeira vari√°vel:", df.columns)
                var2 = st.selectbox("Segunda vari√°vel:", df.columns)
                
                if st.button("üîç An√°lise Bivariada"):
                    analysis = ai.analyze_bivariate(df, var1, var2, target_col)
                    
                    st.subheader(f"An√°lise: {var1} vs {var2}")
                    
                    if 'correlacao' in analysis:
                        st.metric("Correla√ß√£o", f"{analysis['correlacao']:.3f}")
                    
                    if 'correlacao_target' in analysis:
                        st.metric("Correla√ß√£o com Target", f"{analysis['correlacao_target']:.3f}")
                    
                    st.write("**üí° Insights da IA:**")
                    st.info(analysis['sugestao_ia'])
            
            with col2:
                if 'var1' in locals() and 'var2' in locals():
                    if df[var1].dtype in ['int64', 'float64'] and df[var2].dtype in ['int64', 'float64']:
                        fig = px.scatter(df, x=var1, y=var2, color=target_col if target_col else None)
                    else:
                        # Crosstab para categ√≥ricas
                        crosstab = pd.crosstab(df[var1], df[var2])
                        fig = px.imshow(crosstab, text_auto=True)
                    st.plotly_chart(fig, use_container_width=True)
        
        with tab3:
            st.header("ü§ñ Estrat√©gia de Modelagem")
            
            # Sugest√µes estrat√©gicas
            suggestions = ai.suggest_model_strategy(df, target_col)
            
            st.subheader("üí° Sugest√µes da IA para sua modelagem:")
            for suggestion in suggestions:
                st.write(f"‚Ä¢ {suggestion}")
            
            # Sele√ß√£o de modelo
            st.subheader("üéØ Recomenda√ß√µes de Modelos:")
            for model, description in ai.model_suggestions.items():
                st.write(f"**{model.replace('_', ' ').title()}:** {description}")
            
            # Bot√£o para executar modelagem simples
            if st.button("üöÄ Executar Modelagem Demonstrativa"):
                with st.spinner("Treinando modelo..."):
                    # Prepara√ß√£o simplificada dos dados
                    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
                    if target_col in numeric_cols:
                        numeric_cols.remove(target_col)
                    
                    X = df[numeric_cols].fillna(df[numeric_cols].median())
                    y = df[target_col]
                    
                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
                    
                    # Modelo simples
                    model = LogisticRegression(random_state=42)
                    model.fit(X_train, y_train)
                    
                    y_pred = model.predict(X_test)
                    y_proba = model.predict_proba(X_test)[:, 1]
                    
                    # Salvar resultados
                    st.session_state.model_results = {
                        'y_test': y_test,
                        'y_pred': y_pred,
                        'y_proba': y_proba,
                        'model': model,
                        'feature_names': numeric_cols
                    }
                    
                    st.success("‚úÖ Modelo treinado com sucesso!")
        
        with tab4:
            st.header("üìà An√°lise de Performance")
            
            if 'model_results' in st.session_state:
                results = st.session_state.model_results
                
                # Insights da IA
                insights = ai.generate_performance_insights(
                    results['y_test'], 
                    results['y_pred'], 
                    results['y_proba']
                )
                
                st.subheader("üí° Insights da IA sobre Performance:")
                for insight in insights:
                    st.write(f"‚Ä¢ {insight}")
                
                # M√©tricas visuais
                col1, col2 = st.columns(2)
                
                with col1:
                    # ROC Curve
                    from sklearn.metrics import roc_curve
                    fpr, tpr, _ = roc_curve(results['y_test'], results['y_proba'])
                    
                    fig = go.Figure()
                    fig.add_trace(go.Scatter(x=fpr, y=tpr, name='ROC Curve'))
                    fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='Random'))
                    fig.update_layout(title='Curva ROC', xaxis_title='Taxa de Falsos Positivos', yaxis_title='Taxa de Verdadeiros Positivos')
                    st.plotly_chart(fig, use_container_width=True)
                
                with col2:
                    # Matriz de Confus√£o
                    cm = confusion_matrix(results['y_test'], results['y_pred'])
                    fig = px.imshow(cm, text_auto=True, title='Matriz de Confus√£o')
                    st.plotly_chart(fig, use_container_width=True)
            
            else:
                st.info("Execute a modelagem primeiro na aba 'Modelagem'")
        
        with tab5:
            st.header("üìã Pol√≠ticas de Cr√©dito")
            
            if 'model_results' in st.session_state:
                # Sugest√µes de pol√≠ticas
                policies = ai.suggest_credit_policies(
                    st.session_state.model_results, 
                    business_context
                )
                
                st.subheader("ü§ñ Recomenda√ß√µes da IA:")
                for policy in policies:
                    if policy.startswith('üéØ') or policy.startswith('üìã'):
                        st.subheader(policy)
                    else:
                        st.write(policy)
                
                # Simulador de pol√≠ticas
                st.subheader("üéÆ Simulador de Pol√≠ticas")
                
                threshold = st.slider("Threshold de aprova√ß√£o:", 0.0, 1.0, 0.5, 0.01)
                
                if st.button("üîç Simular Pol√≠tica"):
                    y_pred_policy = (st.session_state.model_results['y_proba'] >= threshold).astype(int)
                    
                    # M√©tricas da pol√≠tica
                    from sklearn.metrics import precision_score, recall_score
                    precision = precision_score(st.session_state.model_results['y_test'], y_pred_policy)
                    recall = recall_score(st.session_state.model_results['y_test'], y_pred_policy)
                    approval_rate = y_pred_policy.mean()
                    
                    col1, col2, col3 = st.columns(3)
                    col1.metric("Taxa de Aprova√ß√£o", f"{approval_rate:.1%}")
                    col2.metric("Precis√£o", f"{precision:.3f}")
                    col3.metric("Recall", f"{recall:.3f}")
                    
                    st.info(f"üí° Com threshold {threshold:.2f}, voc√™ aprovaria {approval_rate:.1%} dos clientes com precis√£o de {precision:.3f}")
            
            else:
                st.info("Execute a modelagem primeiro para gerar recomenda√ß√µes de pol√≠ticas")
    
    else:
        st.info("üëÜ Fa√ßa upload de um dataset CSV para come√ßar a an√°lise")
        
        # Exemplo de dataset
        st.subheader("üìù Formato esperado do dataset:")
        example_data = {
            'idade': [25, 35, 45, 30],
            'renda': [3000, 5000, 8000, 4000],
            'score_bureau': [650, 720, 800, 680],
            'tempo_emprego': [12, 36, 60, 24],
            'inadimplente': [0, 0, 0, 1]
        }
        st.dataframe(pd.DataFrame(example_data))
        st.caption("* A coluna 'inadimplente' deve ser bin√°ria (0=bom pagador, 1=mau pagador)")

if __name__ == "__main__":
    main()
